Overview

This repository provides a hands-on approach to understanding backpropagation and gradient computation using Micrograd, a minimalistic educational deep learning library created by Andrej Karpathy. Through these exercises, youâ€™ll explore the core concepts of automatic differentiation, computational graphs, and how frameworks like PyTorch handle gradient-based optimization.


Features

    Custom Value Class: Implement your own computational graph to support basic arithmetic operations and automatic differentiation.
    Softmax and Negative Log-Likelihood: Understand and implement the softmax function and negative log-likelihood loss, which are fundamental in classification tasks.
    Gradient Verification: Compare manually computed gradients with those from PyTorch to ensure correctness.
    Educational Focus: Learn the inner workings of backpropagation and gradient computation in a clear and simplified manner.
